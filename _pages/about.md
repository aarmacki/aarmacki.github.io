---
permalink: /
title: "About"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I hold a PhD degree in electrical and computer engineering from Carnegie Mellon University, where I had the privilege of being advised by prof. [Soummya Kar](https://users.ece.cmu.edu/~soummyak/index.html). Broadly, my interests lie in signal processing and machine learning. More specifically, I study theoretical guarantees of both centralized and large-scale multi-agent learning systems, in the presence of phenomena such as heavy-tailed noise and statistical heterogeneity between different users' datasets.
 
If you have any questions about my research, feel free to [reach out](mailto:aarmacki@andrew.cmu.edu).

News
====
* July 2025 - A new preprint is out. Title: ["Optimal High-probability Convergence of Nonlinear SGD under Heavy-tailed Noise via Symmetrization"](https://arxiv.org/abs/2507.09093).
* Jun 2025 - I defended my PhD thesis! A big thank you to my advisor and committee members, as well as the many people that I had the pleasure of collaborating with during my PhD studies. I will be joining EPFL as a post-doctoral researcher, hosted by professor [Ali H. Sayed](https://asl.epfl.ch/). Looking forward to getting started in September!
* May 2025 - Our paper ["Toward Understanding the Improved Robustness to Initialization in Distributed Clustering"](https://github.com/aarmacki/aarmacki.github.io/blob/master/publications/dist_clust_init.pdf) got accepted at EUSIPCO2025. See you in Palermo! 
* January 2025 - Our paper ["High-probability Convergence Bounds for Online Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise''](https://arxiv.org/abs/2410.13954) got accepted at the International Conference on Artificial Intelligence and Statistics (AISTATS).
* January 2025 - Our paper ["Distributed Center-based Clustering: A Unified Framework''](https://arxiv.org/abs/2402.01302) got accepted at IEEE Transactions on Signal Processing.
* October 2024 - A new preprint is out. Title: ["Large Deviations and Improved Mean-squared Error Rates of Nonlinear SGD: Heavy-tailed Noise and Power of Symmetry''](https://arxiv.org/abs/2410.15637).
* October 2024 - A new preprint is out. Title: ["Nonlinear Stochastic Gradient Descent and Heavy-tailed Noise: A Unified Framework and High-probability Guarantees''](https://arxiv.org/abs/2410.13954).
* February 2024 - A new preprint is out. Title: ["A Unified Framework for Gradient-based Clustering of Distributed Data''](https://arxiv.org/abs/2402.01302).
* December 2023 - Our paper ["A One-shot Framework for Distributed Clustered Learning in Heterogeneous Environments''](https://arxiv.org/abs/2209.10866) got accepted at IEEE Transactions on Signal Processing.
